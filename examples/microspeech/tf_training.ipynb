{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "protecting-wrapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.python.ops import gen_audio_ops as audio_ops\n",
    "from tensorflow.python.ops import io_ops\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.util import compat\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import scipy.io.wavfile as wavfile\n",
    "import tensorflow as tf\n",
    "import os, sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python.ops import io_ops\n",
    "from tf_speech_commands import input_data, models\n",
    "\n",
    "try:\n",
    "    from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op\n",
    "except ImportError:\n",
    "    frontend_op = None\n",
    "    \n",
    "import proc_blocks # Works after running \"maturin develop\" on command line in rune/proc_blocks/python\n",
    "    \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "numeric-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANTED_WORDS = \"yles,alla,parem,vasak\"\n",
    "#DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "\n",
    "DATA_URL = 'https://drive.google.com/uc?export=download&id=1uqR-_u3kxD3u64_MWAq9Gk2_lHT87Oqr' # Commands in Estonian\n",
    "DATASET_DIR =  'recordings_wav/recordings_wav'\n",
    "    \n",
    "PREPROCESS = 'micro'\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "CLIP_DURATION_MS = 1000\n",
    "WINDOW_SIZE_MS = 30.0\n",
    "WINDOW_STRIDE = 20\n",
    "FEATURE_BIN_COUNT = 40\n",
    "BACKGROUND_FREQUENCY = 0.0 # 0.8\n",
    "BACKGROUND_VOLUME_RANGE = 0.0 #0.1\n",
    "TIME_SHIFT_MS = 100.0\n",
    "\n",
    "model_settings = models.prepare_model_settings(\n",
    "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
    "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
    "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS,\n",
    "    enable_pcan=False, min_signal_remaining=1.0,\n",
    "    upper_mel_band_limit=7999.0,\n",
    "    lower_mel_band_limit=0.0)\n",
    "\n",
    "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
    "# to ensure that we have equal number of samples for each label.\n",
    "number_of_labels = WANTED_WORDS.count(',') + 1\n",
    "number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label\n",
    "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
    "\n",
    "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
    "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
    "\n",
    "VALIDATION_PERCENTAGE = 10\n",
    "TESTING_PERCENTAGE = 10\n",
    "LOGS_DIR = 'logs/'\n",
    "TRAIN_DIR = \"train\"\n",
    "\n",
    "MODEL_ARCHITECTURE = 'tiny_conv'\n",
    "#TRAINING_STEPS = \"12000,3000\"\n",
    "#TRAINING_STEPS = \"1200,300\"\n",
    "TRAINING_STEPS = \"12,3\"\n",
    "LEARNING_RATE = \"0.001,0.0001\"\n",
    "\n",
    "EVAL_STEP_INTERVAL = 1000\n",
    "SAVE_STEP_INTERVAL = 1000\n",
    "\n",
    "VERBOSITY = 'WARN'\n",
    "\n",
    "\n",
    "audio_processor = input_data.AudioProcessor(DATA_URL, DATASET_DIR, SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
    "                                            WANTED_WORDS.split(','), VALIDATION_PERCENTAGE, TESTING_PERCENTAGE, \n",
    "                                            model_settings, LOGS_DIR)\n",
    "\n",
    "params = {\n",
    "      'data_url':DATA_URL,\n",
    "      'data_dir':DATASET_DIR,\n",
    "      'background_volume': 0.1, # How loud the background noise should be, between 0 and 1.\n",
    "      'background_frequency':0.8,\n",
    "      'silence_percentage':SILENT_PERCENTAGE,\n",
    "      'unknown_percentage':UNKNOWN_PERCENTAGE,\n",
    "      'time_shift_ms':TIME_SHIFT_MS,\n",
    "      'testing_percentage':TESTING_PERCENTAGE,\n",
    "      'validation_percentage':VALIDATION_PERCENTAGE,\n",
    "      'sample_rate':SAMPLE_RATE,\n",
    "      'clip_duration_ms':CLIP_DURATION_MS,\n",
    "      'window_size_ms': WINDOW_SIZE_MS,\n",
    "      'window_stride_ms':WINDOW_STRIDE,\n",
    "      'feature_bin_count':FEATURE_BIN_COUNT,\n",
    "      'how_many_training_steps':TRAINING_STEPS,\n",
    "      'eval_step_interval':EVAL_STEP_INTERVAL,\n",
    "      'learning_rate':LEARNING_RATE,\n",
    "      'batch_size':16,\n",
    "      'summaries_dir':LOGS_DIR,\n",
    "      'wanted_words':WANTED_WORDS,\n",
    "      'train_dir':TRAIN_DIR,\n",
    "      'save_step_interval':SAVE_STEP_INTERVAL,\n",
    "      'start_checkpoint':\"\",\n",
    "      'model_architecture':MODEL_ARCHITECTURE,\n",
    "      'check_nans':False,\n",
    "      'quantize':True,\n",
    "      'preprocess':PREPROCESS,\n",
    "      'verbosity':VERBOSITY,\n",
    "      'optimizer':'gradient_descent'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "numerous-trade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'desired_samples': 16000,\n",
       " 'window_size_samples': 480,\n",
       " 'window_stride_samples': 320,\n",
       " 'spectrogram_length': 49,\n",
       " 'fingerprint_width': 40,\n",
       " 'fingerprint_size': 1960,\n",
       " 'label_count': 6,\n",
       " 'sample_rate': 16000,\n",
       " 'preprocess': 'micro',\n",
       " 'average_window_width': -1,\n",
       " 'upper_mel_band_limit': 7999.0,\n",
       " 'lower_mel_band_limit': 0.0,\n",
       " 'enable_pcan': False,\n",
       " 'min_signal_remaining': 1.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forced-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.platform import gfile\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "def train(audio_processor, model_settings, params):\n",
    "\n",
    "    tf.compat.v1.logging.set_verbosity(params[\"verbosity\"])\n",
    "\n",
    "    # Start a new TensorFlow session.\n",
    "    sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "    fingerprint_size = model_settings['fingerprint_size']\n",
    "    label_count = model_settings['label_count']\n",
    "    time_shift_samples = int(\n",
    "        (params[\"time_shift_ms\"] * params[\"sample_rate\"]) / 1000)\n",
    "    # Figure out the learning rates for each training phase. Since it's often\n",
    "    # effective to have high learning rates at the start of training, followed by\n",
    "    # lower levels towards the end, the number of steps and learning rates can be\n",
    "    # specified as comma-separated lists to define the rate at each stage. For\n",
    "    # example --how_many_training_steps=10000,3000 --learning_rate=0.001,0.0001\n",
    "    # will run 13,000 training loops in total, with a rate of 0.001 for the first\n",
    "    # 10,000, and 0.0001 for the final 3,000.\n",
    "    training_steps_list = list(\n",
    "        map(int, params[\"how_many_training_steps\"].split(',')))\n",
    "    learning_rates_list = list(map(float, params[\"learning_rate\"].split(',')))\n",
    "    if len(training_steps_list) != len(learning_rates_list):\n",
    "        raise Exception(\n",
    "            '--how_many_training_steps and --learning_rate must be equal length '\n",
    "            'lists, but are %d and %d long instead' % (len(training_steps_list),\n",
    "                                                       len(learning_rates_list)))\n",
    "\n",
    "    input_placeholder = tf.compat.v1.placeholder(\n",
    "        tf.float32, [None, fingerprint_size], name='fingerprint_input')\n",
    "    if params[\"quantize\"]:\n",
    "        fingerprint_min, fingerprint_max = input_data.get_features_range(\n",
    "            model_settings)\n",
    "        fingerprint_input = tf.quantization.fake_quant_with_min_max_args(\n",
    "            input_placeholder, fingerprint_min, fingerprint_max)\n",
    "    else:\n",
    "        fingerprint_input = input_placeholder\n",
    "\n",
    "    logits, dropout_rate = models.create_model(\n",
    "        fingerprint_input,\n",
    "        model_settings,\n",
    "        params[\"model_architecture\"],\n",
    "        is_training=True)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    ground_truth_input = tf.compat.v1.placeholder(\n",
    "        tf.int64, [None], name='groundtruth_input')\n",
    "\n",
    "    # Optionally we can add runtime checks to spot when NaNs or other symptoms of\n",
    "    # numerical errors start occurring during training.\n",
    "    control_dependencies = []\n",
    "    if params[\"check_nans\"]:\n",
    "        checks = tf.compat.v1.add_check_numerics_ops()\n",
    "        control_dependencies = [checks]\n",
    "\n",
    "    # Create the back propagation and training evaluation machinery in the graph.\n",
    "    with tf.compat.v1.name_scope('cross_entropy'):\n",
    "        cross_entropy_mean = tf.compat.v1.losses.sparse_softmax_cross_entropy(\n",
    "            labels=ground_truth_input, logits=logits)\n",
    "\n",
    "    if params[\"quantize\"]:\n",
    "        try:\n",
    "            tf.contrib.quantize.create_training_graph(quant_delay=0)\n",
    "        except AttributeError as e:\n",
    "            msg = e.args[0]\n",
    "            msg += ('\\n\\n The --quantize option still requires contrib, which is not '\n",
    "                    'part of TensorFlow 2.0. Please install a previous version:'\n",
    "                    '\\n    `pip install tensorflow<=1.15`')\n",
    "            e.args = (msg,)\n",
    "            raise e\n",
    "\n",
    "    with tf.compat.v1.name_scope('train'), tf.control_dependencies(\n",
    "            control_dependencies):\n",
    "        learning_rate_input = tf.compat.v1.placeholder(\n",
    "            tf.float32, [], name='learning_rate_input')\n",
    "        if params[\"optimizer\"] == 'gradient_descent':\n",
    "            train_step = tf.compat.v1.train.GradientDescentOptimizer(\n",
    "                learning_rate_input).minimize(cross_entropy_mean)\n",
    "        elif params[\"optimizer\"] == 'momentum':\n",
    "            train_step = tf.compat.v1.train.MomentumOptimizer(\n",
    "                learning_rate_input, .9,\n",
    "                use_nesterov=True).minimize(cross_entropy_mean)\n",
    "        else:\n",
    "            raise Exception('Invalid Optimizer')\n",
    "    predicted_indices = tf.argmax(input=logits, axis=1)\n",
    "    correct_prediction = tf.equal(predicted_indices, ground_truth_input)\n",
    "    confusion_matrix = tf.math.confusion_matrix(labels=ground_truth_input,\n",
    "                                                predictions=predicted_indices,\n",
    "                                                num_classes=label_count)\n",
    "    evaluation_step = tf.reduce_mean(input_tensor=tf.cast(correct_prediction,\n",
    "                                                          tf.float32))\n",
    "    with tf.compat.v1.get_default_graph().name_scope('eval'):\n",
    "        tf.compat.v1.summary.scalar('cross_entropy', cross_entropy_mean)\n",
    "        tf.compat.v1.summary.scalar('accuracy', evaluation_step)\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    increment_global_step = tf.compat.v1.assign(global_step, global_step + 1)\n",
    "\n",
    "    saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n",
    "\n",
    "    # Merge all the summaries and write them out to /tmp/retrain_logs (by default)\n",
    "    merged_summaries = tf.compat.v1.summary.merge_all(scope='eval')\n",
    "    train_writer = tf.compat.v1.summary.FileWriter(params[\"summaries_dir\"] + '/train',\n",
    "                                                   sess.graph)\n",
    "    validation_writer = tf.compat.v1.summary.FileWriter(\n",
    "        params[\"summaries_dir\"] + '/validation')\n",
    "\n",
    "    tf.compat.v1.global_variables_initializer().run()\n",
    "\n",
    "    start_step = 1\n",
    "\n",
    "    if params[\"start_checkpoint\"]:\n",
    "        models.load_variables_from_checkpoint(sess, params[\"start_checkpoint\"])\n",
    "        start_step = global_step.eval(session=sess)\n",
    "\n",
    "    tf.compat.v1.logging.info('Training from step: %d ', start_step)\n",
    "\n",
    "    # Save graph.pbtxt.\n",
    "    tf.io.write_graph(sess.graph_def, params[\"train_dir\"],\n",
    "                      params[\"model_architecture\"] + '.pbtxt')\n",
    "\n",
    "    # Save list of words.\n",
    "    with gfile.GFile(\n",
    "            os.path.join(params[\"train_dir\"],\n",
    "                         params[\"model_architecture\"] + '_labels.txt'),\n",
    "            'w') as f:\n",
    "        f.write('\\n'.join(audio_processor.words_list))\n",
    "\n",
    "    # Training loop.\n",
    "    training_steps_max = np.sum(training_steps_list)\n",
    "    for training_step in xrange(start_step, training_steps_max + 1):\n",
    "        # Figure out what the current learning rate is.\n",
    "        training_steps_sum = 0\n",
    "        for i in range(len(training_steps_list)):\n",
    "            training_steps_sum += training_steps_list[i]\n",
    "            if training_step <= training_steps_sum:\n",
    "                learning_rate_value = learning_rates_list[i]\n",
    "                break\n",
    "        # Pull the audio samples we'll use for training.\n",
    "        train_fingerprints, train_ground_truth = audio_processor.get_data(\n",
    "            params[\"batch_size\"], 0, model_settings, params[\"background_frequency\"],\n",
    "            params[\"background_volume\"], time_shift_samples, 'training', sess)\n",
    "        # Run the graph with this batch of training data.\n",
    "        train_summary, train_accuracy, cross_entropy_value, _, _ = sess.run(\n",
    "            [\n",
    "                merged_summaries,\n",
    "                evaluation_step,\n",
    "                cross_entropy_mean,\n",
    "                train_step,\n",
    "                increment_global_step,\n",
    "            ],\n",
    "            feed_dict={\n",
    "                fingerprint_input: train_fingerprints,\n",
    "                ground_truth_input: train_ground_truth,\n",
    "                learning_rate_input: learning_rate_value,\n",
    "                dropout_rate: 0.5\n",
    "            })\n",
    "        train_writer.add_summary(train_summary, training_step)\n",
    "        tf.compat.v1.logging.debug(\n",
    "            'Step #%d: rate %f, accuracy %.1f%%, cross entropy %f' %\n",
    "            (training_step, learning_rate_value, train_accuracy * 100,\n",
    "             cross_entropy_value))\n",
    "        is_last_step = (training_step == training_steps_max)\n",
    "        if (training_step % params[\"eval_step_interval\"]) == 0 or is_last_step:\n",
    "            tf.compat.v1.logging.info(\n",
    "                'Step #%d: rate %f, accuracy %.1f%%, cross entropy %f' %\n",
    "                (training_step, learning_rate_value, train_accuracy * 100,\n",
    "                 cross_entropy_value))\n",
    "            set_size = audio_processor.set_size('validation')\n",
    "            total_accuracy = 0\n",
    "            total_conf_matrix = None\n",
    "            for i in xrange(0, set_size, params[\"batch_size\"]):\n",
    "                validation_fingerprints, validation_ground_truth = (\n",
    "                    audio_processor.get_data(params[\"batch_size\"], i, model_settings, 0.0,\n",
    "                                             0.0, 0, 'validation', sess))\n",
    "                # Run a validation step and capture training summaries for TensorBoard\n",
    "                # with the `merged` op.\n",
    "                validation_summary, validation_accuracy, conf_matrix = sess.run(\n",
    "                    [merged_summaries, evaluation_step, confusion_matrix],\n",
    "                    feed_dict={\n",
    "                        fingerprint_input: validation_fingerprints,\n",
    "                        ground_truth_input: validation_ground_truth,\n",
    "                        dropout_rate: 0.0\n",
    "                    })\n",
    "                validation_writer.add_summary(\n",
    "                    validation_summary, training_step)\n",
    "                batch_size = min(params[\"batch_size\"], set_size - i)\n",
    "                total_accuracy += (validation_accuracy * batch_size) / set_size\n",
    "                if total_conf_matrix is None:\n",
    "                    total_conf_matrix = conf_matrix\n",
    "                else:\n",
    "                    total_conf_matrix += conf_matrix\n",
    "            tf.compat.v1.logging.info(\n",
    "                'Confusion Matrix:\\n %s' % (total_conf_matrix))\n",
    "            tf.compat.v1.logging.info('Step %d: Validation accuracy = %.1f%% (N=%d)' %\n",
    "                                      (training_step, total_accuracy * 100, set_size))\n",
    "\n",
    "        # Save the model checkpoint periodically.\n",
    "        if (training_step % params[\"save_step_interval\"] == 0 or\n",
    "                training_step == training_steps_max):\n",
    "            checkpoint_path = os.path.join(params[\"train_dir\"],\n",
    "                                           params[\"model_architecture\"] + '.ckpt')\n",
    "            tf.compat.v1.logging.info('Saving to \"%s-%d\"', checkpoint_path,\n",
    "                                      training_step)\n",
    "            saver.save(sess, checkpoint_path, global_step=training_step)\n",
    "\n",
    "    set_size = audio_processor.set_size('testing')\n",
    "    tf.compat.v1.logging.info('set_size=%d', set_size)\n",
    "    total_accuracy = 0\n",
    "    total_conf_matrix = None\n",
    "    for i in xrange(0, set_size, params[\"batch_size\"]):\n",
    "        test_fingerprints, test_ground_truth = audio_processor.get_data(\n",
    "            params[\"batch_size\"], i, model_settings, 0.0, 0.0, 0, 'testing', sess)\n",
    "        test_accuracy, conf_matrix = sess.run(\n",
    "            [evaluation_step, confusion_matrix],\n",
    "            feed_dict={\n",
    "                fingerprint_input: test_fingerprints,\n",
    "                ground_truth_input: test_ground_truth,\n",
    "                dropout_rate: 0.0\n",
    "            })\n",
    "        batch_size = min(params[\"batch_size\"], set_size - i)\n",
    "        total_accuracy += (test_accuracy * batch_size) / set_size\n",
    "        if total_conf_matrix is None:\n",
    "            total_conf_matrix = conf_matrix\n",
    "        else:\n",
    "            total_conf_matrix += conf_matrix\n",
    "    tf.compat.v1.logging.warn('Confusion Matrix:\\n %s' % (total_conf_matrix))\n",
    "    tf.compat.v1.logging.warn('Final test accuracy = %.1f%% (N=%d)' %\n",
    "                              (total_accuracy * 100, set_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sustainable-kelly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/meelislootus/Documents/hotg/tflite_models/venv_tf1_py3d7d6/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:Confusion Matrix:\n",
      " [[0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 3 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 2 0 0]]\n",
      "WARNING:tensorflow:Final test accuracy = 37.5% (N=8)\n"
     ]
    }
   ],
   "source": [
    "train(audio_processor, model_settings, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-shape",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
